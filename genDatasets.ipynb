{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMDB and Bibsonomy Dataset generation for Team Formation\n",
    "#Karan Vombatkere\n",
    "#Spring 2022\n",
    "\n",
    "#Imports\n",
    "import random, json, time\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfb7c6d",
   "metadata": {},
   "source": [
    "## IMDB Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3720b",
   "metadata": {},
   "source": [
    "### Extract and pre-process IMDB data by movie year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfad1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run IMDB data generation process\n",
    "#INPUT: int movieYear - minimum movie year to consider, to subset data from movies from movieYear - present\n",
    "def extractIMDBDataByYear(movieYear):\n",
    "    startTime = time.perf_counter()\n",
    "    imdb_fp = '/usr3/graduate/kvombat/Team-Formation/Datasets/imdb_datasets/raw_data/'\n",
    "    \n",
    "    #1. Read in principals data\n",
    "    principals_df = pd.read_csv(imdb_fp+'title_principals.tsv', sep='\\t', header=0)\n",
    "    principals_ss = principals_df[principals_df['category'].isin(['director','actor','actress'])]\n",
    "    principals_ss = principals_ss[['tconst','nconst','category']]\n",
    "    \n",
    "    \n",
    "    #2. Read in movie titles\n",
    "    titles_df = pd.read_csv(imdb_fp+'title_basics.tsv', sep='\\t', header=0)\n",
    "    \n",
    "    #Keep only movies with valid genres filter to columns needed, and subset movies >= movieYear\n",
    "    titles_df_movies = titles_df[(titles_df.titleType == 'movie') & (titles_df.genres != '\\\\N')\n",
    "                                 & (titles_df.startYear != '\\\\N')]\n",
    "    titles_df_movies = titles_df_movies[titles_df_movies.startYear.astype(int) >= movieYear]\n",
    "    \n",
    "    #filter to columns needed, and calculate number of genres\n",
    "    movies_df = titles_df_movies[['tconst','primaryTitle','startYear','genres']].reset_index(drop=True)\n",
    "    movies_df['numGenres'] = movies_df.genres.apply(lambda x: len(x.split(',')))\n",
    "    \n",
    "    \n",
    "    #3. Merge principals and movies\n",
    "    principal_title_joined = principals_ss.merge(movies_df, left_on='tconst', right_on='tconst')\n",
    "    print(\"Merged principals, movies for movieYear >= {}, principal_title_joined df shape = {}\"\n",
    "          .format(movieYear, principal_title_joined.shape))\n",
    "    \n",
    "    \n",
    "    #4. Get Genres for each principal grouped and aggregated\n",
    "    genres_grouped = principal_title_joined.groupby(['nconst'])['genres'].apply(lambda x:','.join(x.astype(str))).reset_index()\n",
    "    genres_grouped.rename(columns={'genres':'allGenres'}, inplace=True)\n",
    "    genres_grouped['allGenres'] = genres_grouped.allGenres.str.lower().apply(lambda x: list(set(x.split(','))))\n",
    "    \n",
    "    principal_genres_df = principal_title_joined[['nconst','category']].merge(genres_grouped,\n",
    "                                                                              left_on='nconst', right_on='nconst')\n",
    "    principal_genres_df.drop_duplicates(subset=['nconst'], ignore_index=True, inplace=True)\n",
    "    print(\"Merged principals & grouped genres, principal_genres_df shape = {}\".format(principal_genres_df.shape))\n",
    "    \n",
    "    #5. Add principal names and subset to people born after 1950\n",
    "    names_df = pd.read_csv(imdb_fp+'name_basics.tsv', sep='\\t', header=0)\n",
    "    names_ss = names_df[~names_df.deathYear.str.isnumeric() & names_df.birthYear.str.isnumeric()]\n",
    "    names_recent = names_ss[names_ss.birthYear.astype(int) >= 1950]\n",
    "    names_recent = names_recent[['nconst', 'primaryName']]\n",
    "    genres_names_df = principal_genres_df.merge(names_recent, left_on='nconst', right_on='nconst')\n",
    "    \n",
    "    genres_names_df.drop(columns=['nconst'], inplace=True)\n",
    "    genres_names_df['category'].loc[genres_names_df['category'] == 'actress'] = 'actor'\n",
    "    \n",
    "    genres_names_df['allGenres'] = genres_names_df.allGenres.apply(lambda x: ','.join(x))\n",
    "    \n",
    "    # Get count of genres\n",
    "    genres_names_df['numGenres'] = genres_names_df.allGenres.apply(lambda x: len(x.split(',')))\n",
    "    genres_names_df.sort_values(by=['category'], inplace=True)\n",
    "    genres_names_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"Added Names and Genre counts, genres_names_df shape = {}\"\n",
    "          .format(genres_names_df.shape))\n",
    "    \n",
    "    #6. Extract total counts of genre words\n",
    "    genreCounts = {}\n",
    "    for i, genre_string in enumerate(genres_names_df.allGenres.iteritems()):\n",
    "        genre_words = genre_string[1].split(',')\n",
    "        for w in genre_words:\n",
    "            if genreCounts.get(w) is None:\n",
    "                genreCounts[w] = 1\n",
    "            else:\n",
    "                genreCounts[w] += 1\n",
    "    \n",
    "    # Create Genre ID Dict\n",
    "    genre_id_dict = {}\n",
    "    for indx, genre_name in enumerate(list(genreCounts.keys())):\n",
    "        genre_id_dict[genre_name] = indx\n",
    "        \n",
    "    print(\"Completed pre-processing IMDB dataset for movieYear >=\", movieYear)\n",
    "    \n",
    "    runTime = time.perf_counter() - startTime\n",
    "    print(\"IMDB data generation run time = {:.1f} seconds\".format(runTime))\n",
    "    \n",
    "    return genres_names_df, genreCounts, genre_id_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run extraction and pre-processing code\n",
    "movie_year_imdb = 2020\n",
    "\n",
    "genres_names, counts_dict, id_dict = extractIMDBDataByYear(movie_year_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f467c6aa",
   "metadata": {},
   "source": [
    "### Generate expert (directors) skills lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(genre_string):\n",
    "    skill_ids = []\n",
    "    genre_strings_list = genre_string.split(',')\n",
    "\n",
    "    for skill in genre_strings_list:\n",
    "        skill_ids.append(str(id_dict[skill]))\n",
    "    return skill_ids      \n",
    "\n",
    "\n",
    "#Skills list for directors_df\n",
    "def create_expert_skills_list():\n",
    "    directors_df = genres_names[genres_names['category']=='director']\n",
    "    directors_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"Directors skills distribution:\", directors_df.numGenres.describe())\n",
    "    directors_df.hist(column = ['numGenres'], bins=8, figsize=(8,5))\n",
    "    \n",
    "    directors_df['expert_skills'] = directors_df.allGenres.apply(lambda x: extract_skills(x))\n",
    "    expert_skills_list = directors_df.expert_skills.to_list()\n",
    "    \n",
    "    print(\"Successfully generated experts skills list:\", len(expert_skills_list))\n",
    "    return expert_skills_list\n",
    "\n",
    "\n",
    "#Skills list for actors_df\n",
    "def create_tasks_skills_list():\n",
    "    actors_df = genres_names[genres_names['category']=='actor']\n",
    "    actors_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(\"\\nActors skills distribution:\", actors_df.numGenres.describe())\n",
    "    actors_df.hist(column = ['numGenres'], bins=8, figsize=(8,5))\n",
    "    \n",
    "    actors_df['tasks_skills'] = actors_df.allGenres.apply(lambda x: extract_skills(x))\n",
    "    tasks_skills_list = actors_df.tasks_skills.to_list()\n",
    "    \n",
    "    print(\"Successfully generated tasks skills list:\", len(tasks_skills_list))\n",
    "    return tasks_skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf54719",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "expSkills = create_expert_skills_list()\n",
    "taskSkills = create_tasks_skills_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df451bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk\n",
    "imdb_outpath = '/usr3/graduate/kvombat/Team-Formation/Datasets/imdb_datasets/'\n",
    "\n",
    "experts_filename = 'imdb_experts_'+ str(movie_year_imdb) + '.txt'\n",
    "with open(imdb_outpath + experts_filename, 'w') as f:\n",
    "    f.write(json.dumps(expSkills))\n",
    "    \n",
    "tasks_filename = 'imdb_tasks_' + str(movie_year_imdb) + '.txt'\n",
    "with open(imdb_outpath + tasks_filename, 'w') as f:\n",
    "    f.write(json.dumps(taskSkills))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff3086",
   "metadata": {},
   "source": [
    "### Read in final IMDB data from txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05854f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = '/usr3/graduate/kvombat/Team-Formation/Datasets/imdb_datasets/'\n",
    "\n",
    "with open(read_path + 'imdb_experts.txt', 'r') as f:\n",
    "    expert_skills_list = json.loads(f.read())\n",
    "    \n",
    "with open(read_path + 'imdb_tasks.txt', 'r') as f:\n",
    "    task_skills_list = json.loads(f.read())\n",
    "    \n",
    "print(\"Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bfd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e379d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9df6eaef",
   "metadata": {},
   "source": [
    "## Bibsonomy Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99159d",
   "metadata": {},
   "source": [
    "### Extract and pre-process Bibsonomy data by paper year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e41fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "bibsonomy_fp = '/usr3/graduate/kvombat/Team-Formation/Datasets/bibsonomy_datasets/'\n",
    "wordlist_file = open(bibsonomy_fp+\"english_wordlist.txt\")\n",
    "english_words = wordlist_file.read()\n",
    "word_vocab = english_words.split('\\n')\n",
    "\n",
    "def filter_tags(tags_string):\n",
    "    clean_tags_list = []\n",
    "    for tag in tags_string.split(\",\"):\n",
    "        if tag in word_vocab and tag not in stop_words:\n",
    "            clean_tags_list.append(tag)\n",
    "            \n",
    "    clean_tags = \",\".join(f for f in clean_tags_list)\n",
    "    return clean_tags\n",
    "\n",
    "def trim_string(s):\n",
    "    if s[0] == \",\":\n",
    "        s = s[1:]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7362d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run Bibsonomy data generation process\n",
    "#INPUT: int paperYear - minimum paper year to consider, to subset data from papers from paperYear - present\n",
    "def extractBibsonomyDataByYear(paperYear):\n",
    "    startTime = time.perf_counter()\n",
    "    bibsonomy_fp = '/usr3/graduate/kvombat/Team-Formation/Datasets/bibsonomy_datasets/'\n",
    "    \n",
    "    #1. Import tas data and filter to bibtex\n",
    "    tas_df = pd.read_table(bibsonomy_fp+'tas', sep='\\t',usecols=[1,2,3])\n",
    "    tas_df.columns = ['tag', 'content_id', 'content_type']\n",
    "    tas_df = tas_df[tas_df.content_type == 2]\n",
    "    tas_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(\"Imported tas data:\", tas_df.shape)\n",
    "    \n",
    "    #2. Import bibtex data\n",
    "    bibtex_df = pd.read_table(bibsonomy_fp+'bibtex', sep='\\t', usecols=[0,32,34], \n",
    "                              encoding='utf-8', error_bad_lines=False, warn_bad_lines=False, engine='python')\n",
    "    bibtex_df.columns = ['content_id','author','year']\n",
    "    \n",
    "    print(\"Imported bibtex data:\", bibtex_df.shape)\n",
    "    \n",
    "    #Filter NaN data for content_id, author and year\n",
    "    bibtex_df = bibtex_df[~bibtex_df.content_id.isna()]\n",
    "    bibtex_df = bibtex_df[~bibtex_df.author.isna()]\n",
    "    bibtex_df = bibtex_df[~bibtex_df.year.isna()]\n",
    "\n",
    "    #Filter for recent paperYear years\n",
    "    bibtex_df = bibtex_df[bibtex_df.year.str.isdigit()]\n",
    "    bibtex_df.year = bibtex_df.year.astype(int)\n",
    "    bibtex_df = bibtex_df[bibtex_df.year >= paperYear]\n",
    "\n",
    "    #Keep only valid years and content id\n",
    "    bibtex_df = bibtex_df[bibtex_df.content_id.str.isdigit()]\n",
    "    bibtex_df.content_id = bibtex_df.content_id.astype(int)\n",
    "\n",
    "    bibtex_df.reset_index(inplace=True, drop=True)\n",
    "    print(\"Filtered bibtex data to paperYear >= {}, bibtex_df shape: {}\".format(paperYear, bibtex_df.shape))\n",
    "    \n",
    "    \n",
    "    #3. Group and aggregate tags by author\n",
    "    b = pd.DataFrame(bibtex_df.author.str.split(' and ').tolist(), index=bibtex_df.content_id).stack()\n",
    "    b = b.reset_index()[[0, 'content_id']] # var1 variable is currently labeled 0\n",
    "    b.columns = ['author', 'content_id'] # renaming var1\n",
    "    #join author and tags\n",
    "    bibtex_tags_df = b.merge(tas_df, on='content_id')\n",
    "    \n",
    "    #Group and aggregate tags by author\n",
    "    tags_grouped = bibtex_tags_df.groupby(['author'])['tag'].apply(lambda x:','.join(x.astype(str))).reset_index()\n",
    "    tags_grouped.rename(columns={'tag':'allTags'}, inplace=True)\n",
    "    tags_grouped['allTags'] = tags_grouped.allTags.str.lower().apply(lambda x: list(set(x.split(','))))\n",
    "    tags_grouped['allTags'] = tags_grouped.allTags.apply(lambda x: ','.join(x))\n",
    "    \n",
    "    print(\"Grouped and aggregated tags by author, tags_grouped shape: {}\".format(tags_grouped.shape))\n",
    "    \n",
    "    #4. Clean Tags\n",
    "    tags_grouped.allTags = tags_grouped.allTags.apply(lambda x: filter_tags(x))\n",
    "    tags_grouped_clean = tags_grouped[tags_grouped.allTags != '']\n",
    "    tags_grouped_clean.allTags = tags_grouped_clean.allTags.apply(lambda x: trim_string(x))\n",
    "    tags_grouped_clean['numTags'] = tags_grouped_clean.allTags.apply(lambda x: len(x.split(',')))\n",
    "\n",
    "    #Consider authors with only 3 or more tags\n",
    "    tags_grouped_clean = tags_grouped_clean[tags_grouped_clean.numTags >= 3]\n",
    "    print(\"Cleaned tags, tags_grouped_clean shape: {}\".format(tags_grouped_clean.shape))\n",
    "    \n",
    "    #5. Extract total counts of tag words\n",
    "    tagCounts = {}\n",
    "    for i, tag_string in enumerate(tags_grouped_clean.allTags.iteritems()):\n",
    "        tag_words = tag_string[1].split(',')\n",
    "        for w in tag_words:\n",
    "            if tagCounts.get(w) is None:\n",
    "                tagCounts[w] = 1\n",
    "            else:\n",
    "                tagCounts[w] += 1\n",
    "                \n",
    "    #Extract vocab of common tags\n",
    "    tagCounts_list = [(k,v) for k,v in tagCounts.items()]\n",
    "    tagCounts_list.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Len tags count list:\", len(tagCounts_list))\n",
    "    tags_vocab_list = [x[0] for x in tagCounts_list[1:1000]]\n",
    "    \n",
    "    def filter_tags_subset(tags_string):\n",
    "        clean_tags_list = []\n",
    "        for tag in tags_string.split(\",\"):\n",
    "            if tag in tags_vocab_list:\n",
    "                clean_tags_list.append(tag)\n",
    "\n",
    "        clean_tags = \",\".join(f for f in clean_tags_list)\n",
    "        return clean_tags\n",
    "    \n",
    "    tags_grouped_clean.allTags = tags_grouped_clean.allTags.apply(lambda x: filter_tags_subset(x))\n",
    "    tags_grouped_clean = tags_grouped_clean[tags_grouped_clean.allTags != '']\n",
    "    tags_grouped_clean['numTags'] = tags_grouped_clean.allTags.apply(lambda x: len(x.split(',')))\n",
    "    \n",
    "    tags_grouped_clean = tags_grouped_clean[(tags_grouped_clean.numTags >= 3)\n",
    "                                            & (tags_grouped_clean.numTags <= 100)]\n",
    "    tags_grouped_clean.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print(\"Filtered tags on common tags, tags_grouped_clean shape: {}\".format(tags_grouped_clean.shape))\n",
    "\n",
    "    runTime = time.perf_counter() - startTime\n",
    "    print(\"Bibsonomy data generation run time = {:.1f} seconds\".format(runTime))\n",
    "    \n",
    "    return tags_grouped_clean, tags_vocab_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1495be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run extraction and pre-processing code\n",
    "paper_year_bibsonomy = 2000\n",
    "\n",
    "bibtex_tags, tags_vocab = extractBibsonomyDataByYear(paper_year_bibsonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9e1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bibtex_tags.numTags.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734ffca4",
   "metadata": {},
   "source": [
    "### Generate bibsonomy expert and task skills lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcaea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prolific_threshold = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts_df = bibtex_tags[bibtex_tags.numTags >= prolific_threshold]\n",
    "tasks_df = bibtex_tags[bibtex_tags.numTags < prolific_threshold]\n",
    "print(experts_df.shape, tasks_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ca6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts_df.hist(column=['numTags'], bins=10, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a526803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_df.hist(column=['numTags'], bins=5, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc73645",
   "metadata": {},
   "outputs": [],
   "source": [
    "bibtex_tags.hist(column=['numTags'], bins=20, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_id_dict = {}\n",
    "for indx, tag_name in enumerate(tags_vocab):\n",
    "    tag_id_dict[tag_name] = indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(tag_string):\n",
    "    skill_ids = []\n",
    "    tag_strings_list = tag_string.split(',')\n",
    "\n",
    "    for skill in tag_strings_list:\n",
    "        skill_ids.append(str(tag_id_dict[skill]))\n",
    "    return skill_ids      \n",
    "\n",
    "\n",
    "#Skills list for experts - prolific authors\n",
    "def create_expert_skills_list():\n",
    "    experts_df['expert_skills'] = experts_df.allTags.apply(lambda x: extract_skills(x))\n",
    "    expert_skills_list = experts_df.expert_skills.to_list()\n",
    "    \n",
    "    return expert_skills_list\n",
    "\n",
    "\n",
    "#Skills list for actors_df\n",
    "def create_tasks_skills_list():\n",
    "    tasks_df['tasks_skills'] = tasks_df.allTags.apply(lambda x: extract_skills(x))\n",
    "    tasks_skills_list = tasks_df.tasks_skills.to_list()\n",
    "    \n",
    "    return tasks_skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expSkills = create_expert_skills_list()\n",
    "taskSkills = create_tasks_skills_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disk\n",
    "bibsonomy_outpath = '/usr3/graduate/kvombat/Team-Formation/Datasets/bibsonomy_datasets/'\n",
    "\n",
    "experts_filename = 'bibsonomy_experts_'+ str(paper_year_bibsonomy) + '.txt'\n",
    "with open(bibsonomy_outpath + experts_filename, 'w') as f:\n",
    "    f.write(json.dumps(expSkills))\n",
    "    \n",
    "tasks_filename = 'bibsonomy_tasks_'+ str(paper_year_bibsonomy) + '.txt'\n",
    "with open(bibsonomy_outpath + tasks_filename, 'w') as f:\n",
    "    f.write(json.dumps(taskSkills))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2490dd2",
   "metadata": {},
   "source": [
    "### Test Read in final bibsonomy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = '/usr3/graduate/kvombat/Team-Formation/Datasets/bibsonomy_datasets/'\n",
    "\n",
    "with open(read_path + 'bibsonomy_experts.txt', 'r') as f:\n",
    "    expert_skills_list = json.loads(f.read())\n",
    "    \n",
    "with open(read_path + 'bibsonomy_tasks.txt', 'r') as f:\n",
    "    task_skills_list = json.loads(f.read())\n",
    "    \n",
    "print(\"Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e0178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290b917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
