{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Team Formation framework with real datasets\n",
    "## Balancing Task Coverage vs. Maximum Expert Load\n",
    "## Karan Vombatkere, Spring 2022\n",
    "\n",
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json, time, random\n",
    "import TeamFormationProblem as TFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freelancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Freelancer data\n",
    "#Freelancer from DropBox link: https://www.dropbox.com/sh/8zpsi1etvvvvj5k/AAD-J9ZQmSsbnSmEILBMD9uxa/datasets/real?dl=0&subfolder_nav_tracking=1\n",
    "#freelance_experts.csv and freelance_projects.csv\n",
    "\n",
    "def extract_skills(row):\n",
    "    skills = []\n",
    "    for i,val in enumerate(row):\n",
    "        if val == 1:\n",
    "            skills.append(str(i))\n",
    "    return skills            \n",
    "\n",
    "    \n",
    "def importFreelancerData(experts_filename='datasets/freelancer/freelancer_experts.csv', tasks_filename='datasets/freelancer/freelancer_projects.csv'):\n",
    "    #Extract tasks skills as list\n",
    "    freelance_tasks_df = pd.read_csv(tasks_filename, header=None)\n",
    "    print(\"Freelancer tasks df shape: \", freelance_tasks_df.shape)\n",
    "    freelance_tasks_df['Task_Skills'] = freelance_tasks_df.apply(lambda row: extract_skills(row), axis=1)\n",
    "    task_skills_list = freelance_tasks_df.Task_Skills.to_list()\n",
    "    \n",
    "    #Extract experts skills as list\n",
    "    freelance_experts_df = pd.read_csv(experts_filename, header=None)\n",
    "    print(\"Freelancer experts df shape: \", freelance_experts_df.shape)\n",
    "    freelance_experts_df['Expert_Skills'] = freelance_experts_df.apply(lambda row: extract_skills(row), axis=1)\n",
    "    expert_skills_list = freelance_experts_df.Expert_Skills.to_list()\n",
    "\n",
    "    print(\"Imported Freelancer dataset. Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))\n",
    "\n",
    "    return task_skills_list, expert_skills_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = importFreelancerData()\n",
    "FreelancerTest = TFP.TeamFormationProblem(t, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDict, F_vals, workLoad_vals = FreelancerTest.computeTaskAssigment(algorithms=['lazy_greedy', 'random', 'no_update_greedy','task_greedy'], lambdaVal=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taMatList, covLoadList_f = FreelancerTest.setCoverLPTaskCoverage(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_val = 0.1\n",
    "for r, vals in enumerate(covLoadList_f):\n",
    "    print(\"Round {}: Coverage={:.2f}, Max_Load={}, Objective(F)={:.2f}\".format(r+1, vals[0], vals[1], (lambda_val*vals[0] - vals[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tamatlist)):\n",
    "#     sumVal = 0\n",
    "#     for arr in tamatlist[i]:\n",
    "#         sumVal += sum(arr)\n",
    "#     print(sumVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freelancerCovList = FreelancerTest.getCoverageValues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guru Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guru Dataset\n",
    "def extract_skills_guru(row):\n",
    "    skills = []\n",
    "    for i,val in enumerate(row):\n",
    "        if val == 1:\n",
    "            skills.append(str(i))\n",
    "    return skills \n",
    "\n",
    "def importGuruData(experts_filename='datasets/guru/guru_experts.csv', tasks_filename='datasets/guru/guru_tasks.csv'):\n",
    "    #Extract tasks skills as list\n",
    "    guru_tasks_df = pd.read_csv(tasks_filename, header=None)\n",
    "    print(\"Guru tasks df shape: \", guru_tasks_df.shape)\n",
    "    guru_tasks_df['Task_Skills'] = guru_tasks_df.apply(lambda row: extract_skills_guru(row), axis=1)\n",
    "    task_skills_list = guru_tasks_df.Task_Skills.to_list()\n",
    "    task_skills_list = task_skills_list[0:-1]\n",
    "    \n",
    "    #Extract experts skills as list\n",
    "    guru_experts_df = pd.read_csv(experts_filename, header=None)\n",
    "    print(\"Guru experts df shape: \", guru_experts_df.shape)\n",
    "    guru_experts_df['Expert_Skills'] = guru_experts_df.apply(lambda row: extract_skills_guru(row), axis=1)\n",
    "    expert_skills_list = guru_experts_df.Expert_Skills.to_list()\n",
    "    expert_skills_list = expert_skills_list[0:-1]\n",
    "\n",
    "    print(\"Imported Guru dataset. Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))\n",
    "\n",
    "    return task_skills_list, expert_skills_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,e = importGuruData()\n",
    "GuruTest = TFP.TeamFormationProblem(t, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDict, F_vals, workLoad_vals = GuruTest.computeTaskAssigment(algorithms=['no_update_greedy'], lambdaVal=0.1)\n",
    "# runtimeDict, F_vals, workLoad_vals = GuruTest.computeTaskAssigment(algorithms=['lazy_greedy'], lambdaVal=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taMatList, covLoadList = GuruTest.setCoverLPTaskCoverage(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_val = 0.1\n",
    "for r, vals in enumerate(covLoadList):\n",
    "    print(\"Round {}: Coverage={:.2f}, Max_Load={}, Objective(F)={:.2f}\".format(r+1, vals[0], vals[1], (lambda_val*vals[0] - vals[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import IMDB Data\n",
    "def importIMDBData(experts_filename, tasks_filename):\n",
    "    with open(experts_filename, 'r') as f:\n",
    "        expert_skills_list = json.loads(f.read())\n",
    "    \n",
    "    with open(tasks_filename, 'r') as f:\n",
    "        task_skills_list = json.loads(f.read())\n",
    "\n",
    "    print(\"Imported IMDB dataset. Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))\n",
    "\n",
    "    return task_skills_list, expert_skills_list, \n",
    "\n",
    "#Run algorithm on IMDB datasets\n",
    "def testIMDBDatasets(write_flag, algoList):\n",
    "    imdb_data_path = 'datasets/imdb/'\n",
    "    movieYears = [2015, 2018, 2020]\n",
    "\n",
    "    if write_flag:\n",
    "        runTimeStamp = str(time.strftime(\"%m-%d-%H:%M:%S\", time.localtime(time.time())))\n",
    "        imdb_outfilename = \"experiments/imdb_\" + runTimeStamp + \".txt\"\n",
    "        outfile_imdb = open(imdb_outfilename, \"a\")\n",
    "        outfile_imdb.write(\"IMDB dataset Team-Formation Algorithms: {}\\n\".format(runTimeStamp))\n",
    "\n",
    "    for y in movieYears:\n",
    "        experts_file = imdb_data_path + 'imdb_experts_' + str(y) + '.txt'\n",
    "        tasks_file = imdb_data_path + 'imdb_tasks_' + str(y) + '.txt'\n",
    "        print(\"IMDB Dataset: {}, {}\".format('imdb_experts_' + str(y), 'imdb_tasks_' + str(y)))\n",
    "\n",
    "        imdb_tasks, imdb_experts = importIMDBData(experts_file, tasks_file)\n",
    "        IMDBTest = TFP.TeamFormationProblem(imdb_tasks[0:600], imdb_experts[0:100])\n",
    "\n",
    "        rt_dict, f_dict, workload_dict, coverageList = IMDBTest.computeTaskAssigment(algorithms=algoList, plot_flag=False)\n",
    "        coverageListString = \"\"\n",
    "        for c_i in coverageList:\n",
    "            c_i_str = \", \"+str(np.round(c_i, 2))\n",
    "            coverageListString += c_i_str\n",
    "        print(coverageListString)\n",
    "        #Write output to file\n",
    "        if write_flag:\n",
    "            runInfo = \"\\nIMDB movieYear = {}, Experts = {}, Tasks = {}\".format(str(y), str(IMDBTest.n), str(IMDBTest.m))\n",
    "            outfile_imdb.write(runInfo)\n",
    "\n",
    "            f_info = \"\\nAlgorithm Objectives (F_max): Lazy Greedy = {}; No-Update-Greedy = {}; Task Greedy = {}; Random = {};\\\n",
    "                \".format(f_dict['lazyGreedy'], f_dict['noUpdateGreedy'], f_dict['taskGreedy'], f_dict['random'])\n",
    "            outfile_imdb.write(f_info)   \n",
    "\n",
    "            wload_info = \"\\nAlgorithm optimal workloads: Lazy Greedy = {}; No-Update-Greedy = {}; Task Greedy = {}; Random = {};\\\n",
    "                \".format(workload_dict['lazyGreedy'], workload_dict['noUpdateGreedy'], workload_dict['taskGreedy'], workload_dict['random'])\n",
    "            outfile_imdb.write(wload_info)   \n",
    "\n",
    "            runtimeInfo = \"\\nAlgorithm Runtimes: Total = {:.3f}s; Lazy Greedy = {:.3f}s; No-Update-Greedy = {:.3f}s; Task Greedy = {:.3f}s; Random = {:.3f}s;\\\n",
    "                \\n\".format(rt_dict['total'], rt_dict['lazyGreedy'], rt_dict['noUpdateGreedy'], rt_dict['taskGreedy'], rt_dict['random'])\n",
    "            outfile_imdb.write(runtimeInfo)\n",
    "\n",
    "            outfile_imdb.write(\"\\nCoverage List: {}\".format(coverageListString))\n",
    "\n",
    "    \n",
    "    if write_flag:\n",
    "        outfile_imdb.close()\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testIMDBDatasets(write_flag=True, algoList=['lazy_greedy', 'random', 'no_update_greedy', 'task_greedy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 16:50:34,497 |INFO: ------------Team Formation Problem initialized with 13183 tasks and 3871 experts---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Dataset: imdb_experts_2018, imdb_tasks_2018\n",
      "Imported IMDB dataset. Num Experts=3871, Num Tasks=13183\n"
     ]
    }
   ],
   "source": [
    "#Get coverage lists\n",
    "imdb_data_path = 'datasets/imdb/'\n",
    "y = 2018\n",
    "experts_file = imdb_data_path + 'imdb_experts_' + str(y) + '.txt'\n",
    "tasks_file = imdb_data_path + 'imdb_tasks_' + str(y) + '.txt'\n",
    "print(\"IMDB Dataset: {}, {}\".format('imdb_experts_' + str(y), 'imdb_tasks_' + str(y)))\n",
    "\n",
    "imdb_tasks, imdb_experts = importIMDBData(experts_file, tasks_file)\n",
    "IMDBTest = TFP.TeamFormationProblem(imdb_tasks, imdb_experts, max_workload_threshold=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 16:50:58,624 |INFO: Extracted expert and tasks skillset, Total skills = 26\n",
      "2022-10-03 16:50:58,627 |INFO: Generated expert-skill matrix, shape = (3871, 26)\n",
      "2022-10-03 16:50:58,638 |INFO: Generated task-skill matrix, shape = (13183, 26)\n",
      "2022-10-03 17:49:13,778 |INFO: Computing LP solution matrix...\n",
      "2022-10-03 17:56:22,327 |INFO: Running Probabilistic Task Assignment using LP Solution Matrix\n",
      "2022-10-03 18:17:31,097 |INFO: LP solver Assignment computation time = 5192.4 seconds\n"
     ]
    }
   ],
   "source": [
    "taMatList, covLoadList_i = IMDBTest.setCoverLPTaskCoverage(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Coverage=9607.39, Max_Load=74, Objective(F)=406.37\n",
      "Round 2: Coverage=11925.35, Max_Load=122, Objective(F)=474.27\n",
      "Round 3: Coverage=12740.49, Max_Load=174, Objective(F)=463.02\n",
      "Round 4: Coverage=13015.44, Max_Load=217, Objective(F)=433.77\n",
      "Round 5: Coverage=13125.76, Max_Load=252, Objective(F)=404.29\n",
      "Round 6: Coverage=13165.36, Max_Load=282, Objective(F)=376.27\n",
      "Round 7: Coverage=13177.67, Max_Load=309, Objective(F)=349.88\n",
      "Round 8: Coverage=13181.67, Max_Load=331, Objective(F)=328.08\n",
      "Round 9: Coverage=13183.00, Max_Load=350, Objective(F)=309.15\n",
      "Round 10: Coverage=13183.00, Max_Load=370, Objective(F)=289.15\n"
     ]
    }
   ],
   "source": [
    "lambda_val = 0.05\n",
    "for r, vals in enumerate(covLoadList_i):\n",
    "    print(\"Round {}: Coverage={:.2f}, Max_Load={}, Objective(F)={:.2f}\".format(r+1, vals[0], vals[1], (lambda_val*vals[0] - vals[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDict, F_vals, workLoad_vals = IMDBTest.computeTaskAssigment(algorithms=['random', 'no_update_greedy', 'task_greedy'], lambdaVal=0.05)\n",
    "#runtimeDict, F_vals, workLoad_vals = IMDBTest.computeTaskAssigment(algorithms=['no_update_greedy'], lambdaVal=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covDict = IMDBCoverages.getStepCoverageValues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_path = 'datasets/imdb/'\n",
    "y = 2015\n",
    "experts_file = imdb_data_path + 'imdb_experts_' + str(y) + '.txt'\n",
    "tasks_file = imdb_data_path + 'imdb_tasks_' + str(y) + '.txt'\n",
    "print(\"IMDB Dataset: {}, {}\".format('imdb_experts_' + str(y), 'imdb_tasks_' + str(y)))\n",
    "\n",
    "imdb_tasks, imdb_experts = importIMDBData(experts_file, tasks_file)\n",
    "IMDBLambdaTest = TFP.TeamFormationProblem(imdb_tasks[:1000], imdb_experts[300:600], max_workload_threshold=100)\n",
    "\n",
    "t_arr, f_dict, t_maxArr, f_maxArr = IMDBLambdaTest.testLambdaTaskAssignment(algorithms=['lazy_greedy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot F_i for different Lambda for Lazy Greedy\n",
    "plt.figure(figsize=(9,6))\n",
    "#for l_val in f_dict.keys():\n",
    "    #plt.plot(t_arr, f_dict[l_val], label='Lambda={:.3f}'.format(l_val))\n",
    "\n",
    "# Plot the max values\n",
    "plt.plot(t_maxArr, f_maxArr, '--*', label='Max F_i')\n",
    "\n",
    "# title_text = 'Lazy Greedy Performance by varying Lambda (IMDB_2015)'\n",
    "# plt.title(title_text, fontsize=12)\n",
    "plt.xlabel('Workload Threshold, T_i', fontsize=12)\n",
    "plt.ylabel('Coverage, C(A)', fontsize=12)\n",
    "\n",
    "\n",
    "lambda_arr = [0.3, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 100]\n",
    "i=0\n",
    "# zip joins x and y coordinates in pairs\n",
    "for x,y in zip(t_maxArr,f_maxArr):\n",
    "    label = \"{:.1f}\".format(lambda_arr[i])\n",
    "    i += 1\n",
    "    plt.annotate(label, # this is the text\n",
    "                 (x,y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\n",
    "                 ha='center') # horizontal alignment can be left, right or center\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bibsonomy Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Bibsonomy datasets\n",
    "def importBibsonomyData(experts_filename, tasks_filename):\n",
    "    with open(experts_filename, 'r') as f:\n",
    "        expert_skills_list = json.loads(f.read())\n",
    "    \n",
    "    with open(tasks_filename, 'r') as f:\n",
    "        task_skills_list = json.loads(f.read())\n",
    "\n",
    "    print(\"Imported Bibsonomy dataset. Num Experts={}, Num Tasks={}\".format(len(expert_skills_list),len(task_skills_list)))\n",
    "\n",
    "    return task_skills_list, expert_skills_list\n",
    "\n",
    "#Run algorithm on Bibsonomy datasets\n",
    "def testBibsonomyDatasets(write_flag, algoList):\n",
    "    bibsonomy_data_path = 'datasets/bibsonomy/'\n",
    "    movieYears = [2010, 2015, 2020]\n",
    "\n",
    "    if write_flag:\n",
    "        runTimeStamp = str(time.strftime(\"%m-%d-%H:%M:%S\", time.localtime(time.time())))\n",
    "        bibs_outfilename = \"experiments/bibsonomy_\" + runTimeStamp + \".txt\"\n",
    "        outfile_bibsonomy = open(bibs_outfilename, \"a\")\n",
    "        outfile_bibsonomy.write(\"Bibsonomy dataset Team-Formation Algorithms: {}\\n\".format(runTimeStamp))\n",
    "\n",
    "    for y in movieYears:\n",
    "        experts_file = bibsonomy_data_path + 'bibsonomy_experts_' + str(y) + '.txt'\n",
    "        tasks_file = bibsonomy_data_path + 'bibsonomy_tasks_' + str(y) + '.txt'\n",
    "        print(\"\\nBibsonomy Dataset: {}, {}\".format('bibsonomy_experts_' + str(y), 'bibsonomy_tasks_' + str(y)))\n",
    "\n",
    "        bib_tasks, bib_experts = importBibsonomyData(experts_file, tasks_file)\n",
    "        BibsonomyTest = TFP.TeamFormationProblem(bib_tasks[0:500], bib_experts[0:200])\n",
    "\n",
    "        rt_dict, f_dict, workload_dict = BibsonomyTest.computeTaskAssigment(algorithms=algoList, plot_flag=False)\n",
    "\n",
    "        #Write output to file\n",
    "        if write_flag:\n",
    "            runInfo = \"\\nBibsonomy paperYear = {}, Experts = {}, Tasks = {}\".format(str(y), str(BibsonomyTest.n), str(BibsonomyTest.m))\n",
    "            outfile_bibsonomy.write(runInfo)\n",
    "\n",
    "            f_info = \"\\nAlgorithm Objectives (F_max): Lazy Greedy = {}; No-Update-Greedy = {}; Task Greedy = {}; Random = {};\\\n",
    "                \".format(f_dict['lazyGreedy'], f_dict['noUpdateGreedy'], f_dict['taskGreedy'], f_dict['random'])\n",
    "            outfile_bibsonomy.write(f_info)   \n",
    "\n",
    "            wload_info = \"\\nAlgorithm optimal workloads: Lazy Greedy = {}; No-Update-Greedy = {}; Task Greedy = {}; Random = {};\\\n",
    "                \".format(workload_dict['lazyGreedy'], workload_dict['noUpdateGreedy'], workload_dict['taskGreedy'], workload_dict['random'])\n",
    "            outfile_bibsonomy.write(wload_info)   \n",
    "\n",
    "            runtimeInfo = \"\\nAlgorithm Runtimes: Total = {:.3f}s; Lazy Greedy = {:.3f}s; No-Update-Greedy = {:.3f}s; Task Greedy = {:.3f}s; Random = {:.3f}s;\\\n",
    "                \\n\".format(rt_dict['total'], rt_dict['lazyGreedy'], rt_dict['noUpdateGreedy'], rt_dict['taskGreedy'], rt_dict['random'])\n",
    "            outfile_bibsonomy.write(runtimeInfo)\n",
    "    \n",
    "    if write_flag:\n",
    "        outfile_bibsonomy.close()\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testBibsonomyDatasets(write_flag=True, algoList=['lazy_greedy', 'random', 'no_update_greedy', 'task_greedy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 18:19:02,315 |INFO: ------------Team Formation Problem initialized with 21981 tasks and 3044 experts---------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bibsonomy Dataset: bibsonomy_experts_2010, bibsonomy_tasks_2010\n",
      "Imported Bibsonomy dataset. Num Experts=3044, Num Tasks=21981\n"
     ]
    }
   ],
   "source": [
    "bibsonomy_data_path = 'datasets/bibsonomy/'\n",
    "y=2010\n",
    "experts_file = bibsonomy_data_path + 'bibsonomy_experts_' + str(y) + '.txt'\n",
    "tasks_file = bibsonomy_data_path + 'bibsonomy_tasks_' + str(y) + '.txt'\n",
    "print(\"\\nBibsonomy Dataset: {}, {}\".format('bibsonomy_experts_' + str(y), 'bibsonomy_tasks_' + str(y)))\n",
    "\n",
    "bib_tasks, bib_experts = importBibsonomyData(experts_file, tasks_file)\n",
    "BibsonomyTest = TFP.TeamFormationProblem(bib_tasks, bib_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-03 18:19:31,507 |INFO: Extracted expert and tasks skillset, Total skills = 999\n",
      "2022-10-03 18:19:31,536 |INFO: Full task coverage not possible, generated coverage upper bounds for each task\n",
      "2022-10-03 18:19:31,551 |INFO: Generated expert-skill matrix, shape = (3044, 999)\n",
      "2022-10-03 18:19:31,606 |INFO: Generated task-skill matrix, shape = (21981, 999)\n"
     ]
    }
   ],
   "source": [
    "taMatList, covLoadList_b = BibsonomyTest.setCoverLPTaskCoverage(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1: Coverage=7811.97, Max_Load=55, Objective(F)=335.60\n",
      "Round 2: Coverage=8361.22, Max_Load=93, Objective(F)=325.06\n",
      "Round 3: Coverage=8526.12, Max_Load=132, Objective(F)=294.31\n",
      "Round 4: Coverage=8577.18, Max_Load=162, Objective(F)=266.86\n",
      "Round 5: Coverage=8593.24, Max_Load=180, Objective(F)=249.66\n",
      "Round 6: Coverage=8602.92, Max_Load=201, Objective(F)=229.15\n",
      "Round 7: Coverage=8603.95, Max_Load=224, Objective(F)=206.20\n",
      "Round 8: Coverage=8604.20, Max_Load=244, Objective(F)=186.21\n",
      "Round 9: Coverage=8604.20, Max_Load=259, Objective(F)=171.21\n",
      "Round 10: Coverage=8604.20, Max_Load=278, Objective(F)=152.21\n"
     ]
    }
   ],
   "source": [
    "lambda_val = 0.1\n",
    "for r, vals in enumerate(covLoadList_b):\n",
    "    print(\"Round {}: Coverage={:.2f}, Max_Load={}, Objective(F)={:.2f}\".format(r+1, vals[0], vals[1], (lambda_val*vals[0] - vals[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimeDict, F_vals, workLoad_vals = BibsonomyTest.computeTaskAssigment(algorithms=['random', 'no_update_greedy', 'task_greedy'], lambdaVal=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot F_i for different Lambda for Lazy Greedy\n",
    "# plt.figure(figsize=(9,6))\n",
    "# for l_val in Fi_dict.keys():\n",
    "#     plt.plot(T_arr, Fi_dict[l_val], label='Lambda={:.3f}'.format(l_val))\n",
    "\n",
    "# # Plot the max values\n",
    "# plt.plot(TMaxArr, FMaxArr, '--*', label='Max F_i')\n",
    "\n",
    "# title_text = 'Lazy Greedy Performance by varying Lambda (Bibsonomy_2015)'\n",
    "# plt.title(title_text, fontsize=12)\n",
    "# plt.xlabel('Workload Threshold, T_i')\n",
    "# plt.ylabel('F_i')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_threshold_arr = [5,10,40,80,100,150,200]\n",
    "# rev_rt_arr, reg_rt_arr = [],[]\n",
    "# for thresh in max_threshold_arr:\n",
    "#     FreelancerTest = TFP.TeamFormationProblem(t[0:200], e[0:200], max_workload_threshold=thresh)\n",
    "#     rev_rt, reg_rt = FreelancerTest.compare_Methods()\n",
    "#     rev_rt_arr.append(rev_rt)\n",
    "#     reg_rt_arr.append(reg_rt)\n",
    "\n",
    "#Plot Runtimes\n",
    "# plt.figure(figsize=(9,6))\n",
    "# plt.plot(max_threshold_arr, rev_rt_arr, label='Reverse Threshold Runtime')\n",
    "# plt.plot(max_threshold_arr, reg_rt_arr, label='Regular Lazy Runtime')\n",
    "\n",
    "# title_text = 'Reverse Threshold vs. Regular Lazy runtimes'\n",
    "# plt.title(title_text, fontsize=11)\n",
    "# plt.xlabel('Max Threshold, T_i')\n",
    "# plt.ylabel('Runtime, s')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()\n",
    "\n",
    "#FreelancerTest.compute_reverseThreshold()\n",
    "#FreelancerTest.compareTest_Lazy_Stochastic_Assignments()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
